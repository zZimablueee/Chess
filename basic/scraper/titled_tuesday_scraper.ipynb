{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73ee3d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import csv\n",
    "import sys\n",
    "from urllib.parse import urljoin, urlparse, parse_qs, urlencode, urlunparse\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup # --> pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a841b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BASE = \"https://www.chess.com/tournament/live/titled-tuesdays\"\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (compatible; titled-tuesday-archiver/1.2; +https://example.org)\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "}\n",
    "REQUEST_DELAY_SECS = 1.25  # polite delay\n",
    "\n",
    "\n",
    "def get_soup(session, url):\n",
    "    r = session.get(url, headers=HEADERS, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    return BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "\n",
    "def extract_rows_with_date_and_link(soup):\n",
    "    \"\"\"\n",
    "    Extract rows where we can find a date (class 'tournaments-live-date')\n",
    "    and a corresponding tournament link on the same row/container.\n",
    "\n",
    "    Since we don't control the exact HTML structure, we:\n",
    "      1) Find every date node with class 'tournaments-live-date'\n",
    "      2) Walk up to a reasonable parent container (e.g., a row/div/li)\n",
    "      3) Inside that container, look for an <a> to a '/tournament/live/' page\n",
    "         whose slug contains 'titled' (robust filter for Titled Tuesday)\n",
    "    \"\"\"\n",
    "    # results: List[Dict[str, str]] = []\n",
    "    results = []\n",
    "    date_nodes = soup.find_all(attrs={\"class\": lambda c: c and \"tournaments-live-date\" in c.split()})\n",
    "\n",
    "    for date_node in date_nodes:\n",
    "        date_text = date_node.get_text(strip=True)\n",
    "\n",
    "        # Heuristic: find a nearby container that also has the link\n",
    "        container = date_node\n",
    "        for _ in range(4):  # climb a few levels max to find a row container\n",
    "            container = container.find_parent()\n",
    "            if container is None:\n",
    "                break\n",
    "            # try to find a suitable link inside this container\n",
    "            a = container.find(\"a\", href=True)\n",
    "            if a:\n",
    "                hrefs = [a[\"href\"]]\n",
    "                # sometimes there may be multiple anchors in the row; try all of them\n",
    "                hrefs.extend([x[\"href\"] for x in container.find_all(\"a\", href=True)])\n",
    "                # filter candidates\n",
    "                candidates = [\n",
    "                    urljoin(BASE, h)\n",
    "                    for h in hrefs\n",
    "                    if \"/tournament/live/\" in h and \"titled\" in h.lower()\n",
    "                ]\n",
    "                if candidates:\n",
    "                    # pick the first candidate (most rows have one canonical link)\n",
    "                    results.append({\"Date\": date_text, \"tournament_url\": candidates[0]})\n",
    "                    break\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def find_next_page_href(soup):\n",
    "    # rel=\"next\"\n",
    "    a = soup.find(\"a\", rel=lambda v: v and \"next\" in v.lower())\n",
    "    if a and a.get(\"href\"):\n",
    "        return urljoin(BASE, a[\"href\"])\n",
    "    # aria-label or textual indicators\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        label = a.get(\"aria-label\", \"\").lower()\n",
    "        txt = a.get_text(strip=True).lower()\n",
    "        if label == \"next\" or txt in {\"next\", \"older\", \"›\", \"»\"}:\n",
    "            return urljoin(BASE, a[\"href\"])\n",
    "    return None\n",
    "\n",
    "\n",
    "def with_page_param(url, page):\n",
    "    parts = list(urlparse(url))\n",
    "    qs = parse_qs(parts[4])\n",
    "    qs[\"page\"] = [str(page)]\n",
    "    parts[4] = urlencode({k: (v[0] if isinstance(v, list) and len(v) == 1 else v) for k, v in qs.items()})\n",
    "    return urlunparse(parts)\n",
    "\n",
    "\n",
    "def crawl_pages(start_url = BASE, max_pages = 3, start_page = 1):\n",
    "    \"\"\"\n",
    "    Crawl up to 'max_pages' listing pages and return a DataFrame with:\n",
    "        columns = ['Date', 'tournament_url']\n",
    "\n",
    "    Behavior:\n",
    "      - Follow a visible 'Next' link if available.\n",
    "      - Otherwise, probe ?page=N for N=2,3,... until 'max_pages' are fetched.\n",
    "      - Deduplicate rows by (Date, URL).\n",
    "    \"\"\"\n",
    "    session = requests.Session()\n",
    "    seen_pages = set()\n",
    "    rows = []\n",
    "\n",
    "    url = with_page_param(start_url, start_page) if start_page > 1 else start_url\n",
    "    page_count = 0\n",
    "    next_mode = \"auto\"   # 'next' while Next works; fall back to 'probe'\n",
    "    probe_page = max(start_page, 1)\n",
    "\n",
    "    while url and (max_pages is None or page_count < max_pages):\n",
    "        if url in seen_pages:\n",
    "            break\n",
    "        seen_pages.add(url)\n",
    "        page_count += 1\n",
    "\n",
    "        print(f\"[page {page_count}] {url}\", file=sys.stderr)\n",
    "        soup = get_soup(session, url)\n",
    "\n",
    "        page_rows = extract_rows_with_date_and_link(soup)\n",
    "        print(f\"  extracted {len(page_rows)} date/url pairs\", file=sys.stderr)\n",
    "        rows.extend(page_rows)\n",
    "\n",
    "        time.sleep(REQUEST_DELAY_SECS)\n",
    "\n",
    "        # Try a visible Next link first\n",
    "        if next_mode in (\"auto\", \"next\"):\n",
    "            nxt = find_next_page_href(soup)\n",
    "            if nxt and nxt not in seen_pages:\n",
    "                next_mode = \"next\"\n",
    "                url = nxt\n",
    "                continue\n",
    "            # switch to probe mode\n",
    "            next_mode = \"probe\"\n",
    "            try:\n",
    "                cur_q = parse_qs(urlparse(url).query)\n",
    "                cur_p = int(cur_q.get(\"page\", [probe_page])[0])\n",
    "                probe_page = cur_p + 1\n",
    "            except ValueError:\n",
    "                probe_page += 1\n",
    "\n",
    "        # Probe mode via ?page=N\n",
    "        if next_mode == \"probe\":\n",
    "            probe_page += 1 if page_count > 1 else 0\n",
    "            candidate = with_page_param(start_url, probe_page)\n",
    "            url = candidate if candidate not in seen_pages else None\n",
    "\n",
    "    # Build DataFrame and de-duplicate\n",
    "    df = pd.DataFrame(rows, columns=[\"Date\", \"tournament_url\"]).drop_duplicates().reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Example: crawl first 2 pages; adjust as needed\n",
    "    df = crawl_pages(max_pages=2)\n",
    "    print(f\"\\nCollected {len(df)} rows\")\n",
    "    # Save if you want:\n",
    "    out_csv = \"titled_tuesday_dates_and_links.csv\"\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    print(f\"Saved to {out_csv}\")\n",
    "    # Show a quick preview\n",
    "    print(df.head().to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fcaa563",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[page 1] https://www.chess.com/tournament/live/titled-tuesdays\n",
      "  extracted 25 date/url pairs\n",
      "[page 2] https://www.chess.com/tournament/live/titled-tuesdays?page=2\n",
      "  extracted 25 date/url pairs\n",
      "[page 3] https://www.chess.com/tournament/live/titled-tuesdays?page=3\n",
      "  extracted 25 date/url pairs\n",
      "[page 4] https://www.chess.com/tournament/live/titled-tuesdays?page=4\n",
      "  extracted 25 date/url pairs\n",
      "[page 5] https://www.chess.com/tournament/live/titled-tuesdays?page=5\n",
      "  extracted 25 date/url pairs\n",
      "[page 6] https://www.chess.com/tournament/live/titled-tuesdays?page=6\n",
      "  extracted 25 date/url pairs\n",
      "[page 7] https://www.chess.com/tournament/live/titled-tuesdays?page=7\n",
      "  extracted 25 date/url pairs\n",
      "[page 8] https://www.chess.com/tournament/live/titled-tuesdays?page=8\n",
      "  extracted 25 date/url pairs\n",
      "[page 9] https://www.chess.com/tournament/live/titled-tuesdays?page=9\n",
      "  extracted 25 date/url pairs\n",
      "[page 10] https://www.chess.com/tournament/live/titled-tuesdays?page=10\n",
      "  extracted 25 date/url pairs\n",
      "[page 11] https://www.chess.com/tournament/live/titled-tuesdays?page=11\n",
      "  extracted 25 date/url pairs\n",
      "[page 12] https://www.chess.com/tournament/live/titled-tuesdays?page=12\n",
      "  extracted 25 date/url pairs\n",
      "[page 13] https://www.chess.com/tournament/live/titled-tuesdays?page=13\n",
      "  extracted 25 date/url pairs\n",
      "[page 14] https://www.chess.com/tournament/live/titled-tuesdays?page=14\n",
      "  extracted 25 date/url pairs\n",
      "[page 15] https://www.chess.com/tournament/live/titled-tuesdays?page=15\n",
      "  extracted 25 date/url pairs\n",
      "[page 16] https://www.chess.com/tournament/live/titled-tuesdays?page=16\n",
      "  extracted 25 date/url pairs\n",
      "[page 17] https://www.chess.com/tournament/live/titled-tuesdays?page=17\n",
      "  extracted 25 date/url pairs\n",
      "[page 18] https://www.chess.com/tournament/live/titled-tuesdays?page=18\n",
      "  extracted 25 date/url pairs\n",
      "[page 19] https://www.chess.com/tournament/live/titled-tuesdays?page=19\n",
      "  extracted 25 date/url pairs\n",
      "[page 20] https://www.chess.com/tournament/live/titled-tuesdays?page=20\n",
      "  extracted 25 date/url pairs\n",
      "[page 21] https://www.chess.com/tournament/live/titled-tuesdays?page=21\n",
      "  extracted 25 date/url pairs\n",
      "[page 22] https://www.chess.com/tournament/live/titled-tuesdays?page=22\n",
      "  extracted 25 date/url pairs\n",
      "[page 23] https://www.chess.com/tournament/live/titled-tuesdays?page=23\n",
      "  extracted 11 date/url pairs\n",
      "[page 24] https://www.chess.com/tournament/live/titled-tuesdays?page=24\n",
      "  extracted 0 date/url pairs\n"
     ]
    }
   ],
   "source": [
    "links = crawl_pages(BASE, max_pages=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e74a57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "links_df = links\n",
    "links_df['Date'] = pd.to_datetime(links_df.Date)\n",
    "links_df = links_df.sort_values(by='Date', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "afb59e9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>tournament_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-10-21 08:00:00</td>\n",
       "      <td>https://www.chess.com/tournament/live/titled-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-10-14 08:00:00</td>\n",
       "      <td>https://www.chess.com/tournament/live/titled-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-10-07 08:00:00</td>\n",
       "      <td>https://www.chess.com/tournament/live/titled-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-09-30 08:00:00</td>\n",
       "      <td>https://www.chess.com/tournament/live/titled-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-09-23 08:00:00</td>\n",
       "      <td>https://www.chess.com/tournament/live/titled-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>2015-03-03 11:00:00</td>\n",
       "      <td>https://www.chess.com/tournament/live/-titled-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>2015-02-24 11:00:00</td>\n",
       "      <td>https://www.chess.com/tournament/live/-titled-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>2015-01-27 12:30:00</td>\n",
       "      <td>https://www.chess.com/tournament/live/-titled-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>2015-01-27 11:00:00</td>\n",
       "      <td>https://www.chess.com/tournament/live/-titled-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>2014-12-30 11:00:00</td>\n",
       "      <td>https://www.chess.com/tournament/live/-titled-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>561 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Date                                     tournament_url\n",
       "0   2025-10-21 08:00:00  https://www.chess.com/tournament/live/titled-t...\n",
       "1   2025-10-14 08:00:00  https://www.chess.com/tournament/live/titled-t...\n",
       "2   2025-10-07 08:00:00  https://www.chess.com/tournament/live/titled-t...\n",
       "3   2025-09-30 08:00:00  https://www.chess.com/tournament/live/titled-t...\n",
       "4   2025-09-23 08:00:00  https://www.chess.com/tournament/live/titled-t...\n",
       "..                  ...                                                ...\n",
       "556 2015-03-03 11:00:00  https://www.chess.com/tournament/live/-titled-...\n",
       "557 2015-02-24 11:00:00  https://www.chess.com/tournament/live/-titled-...\n",
       "558 2015-01-27 12:30:00  https://www.chess.com/tournament/live/-titled-...\n",
       "559 2015-01-27 11:00:00  https://www.chess.com/tournament/live/-titled-...\n",
       "560 2014-12-30 11:00:00  https://www.chess.com/tournament/live/-titled-...\n",
       "\n",
       "[561 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65b2db23",
   "metadata": {},
   "outputs": [],
   "source": [
    "links_df.to_csv('tournament_link.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
